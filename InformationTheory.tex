\documentclass[a4paper,14pt]{extreport}
\usepackage[utf8]{inputenc}
\usepackage[english,ukrainian]{babel}
\usepackage{tempora}
\usepackage{fancyhdr} 
\usepackage{titlesec}
\usepackage{mathtools}
\usepackage{starfont}
\usepackage{longtable}
\usepackage{graphics}
\usepackage{listings}
\usepackage{caption}
\usepackage{float}
\usepackage{amsmath}
\usepackage{csquotes}
\usepackage[backend=biber]{biblatex}
\addbibresource{refs.bib}
\newenvironment{metaverbatim}{\verbatim}{\endverbatim}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\usepackage{{booktabs}}
\begin{document}
\begin{titlepage}
	\begin{center}
		\large
		МІНІСТЕРСТВО ОСВІТИ І НАУКИ УКРАЇНИ
		
		\vspace{0.5cm}
		
		СУМСЬКИЙ ДЕРЖАВНИЙ УНІВЕРСИТЕТ\\
		
		\vspace{0.25cm}
		
		Кафедра електроніки і комп'ютерної техніки
		\vspace{2.5cm}
		\vfill
		
		\textsc{КОНТРОЛЬНА РОБОТА}\\[2mm]
		
		з дисципліни \\«Теорія інформації та кодування»
		\vfill
		
	
		
	\end{center}
	\vfill
	
	\newlength{\ML}
	\settowidth{\ML}{}
	Виконав аспірант групи Аз-26/КН Хитров О.Б.
	\bigskip
	

	Перевірив О. Б. Бережна
	\vfill
	
	\vspace{5cm}
	
	\begin{center}
		Суми\\ 2023 р.
	\end{center}
	
\end{titlepage}
	
\section{Завдання 1}
	
\subsection{Завадостійкі коди}
	Завадостійкі коди - одне з найбільш ефективних засобів забезпечення високої достовірності передачі дискреетної інформаіції.

Вхідний текст

\subsection{Вхідний текст}


 \begin{figure}[h]
	\centering
	\begin{minipage}{0.8\textwidth}  % Adjust the width as needed
	4 Веста (символ: \Vesta) — астероїд, один із найбільших об'єктів у поясі астероїдів (середній діаметр 525 км). Відкритий 29 березня 1807 року німецьким астроном Генріхом Ольберсом і названий на честь Вести, богині дому та вогнища в римській міфології. Вважається, що Веста є другим об'єктом у поясі астероїдів за масою та за об'ємом після карликової планети Церери, хоч об'єм Вести (у межах похибки вимірювань) дорівнює об'єму Паллади. На Весту припадає близько 9 \% маси поясу астероїдів. Для наземного спостерігача Веста є найяскравішим астероїдом. Її яскравість може досягати 5,1 зоряної величини, і в такі моменти її, хоч і слабко, видно неозброєним оком. 1,2 мільярда років тому Веста зазнала зіткнень з іншими об'єктами, що спричинили формування численних уламків та двох величезних кратерів, які займають більшу частину південної півкулі астероїда. Найбільше інформації про поверхню Вести відомо завдяки американському космічному апарату «Dawn», який дослідив астероїд в 2011—2012 роках
	\end{minipage}
	\caption*{Вхідний текст}
	\label{fig:sample-text}
\end{figure}


Потужність множини $A$ дорівніює кількості символів $a_i$ первинного алфавіту $12$. 

Переведемо вхідний текст у  верхній регістр, та відфільтруємо символи, що не належать до первинного алфавіту


 \begin{figure}[h]
	\centering
	\begin{minipage}{0.8\textwidth}  % Adjust the width as needed
	ЕА МЛ   АЕР,   АЛ  У  АЕР ЕРЕ АМЕР  М. Р  ЕРЕ  РУ МЕЦМ АРМ ГЕРМ ЛЕРМ  АА А Е Е, Г МУ А ГА  РМ МЛГ. АЖА,  ЕА  РУГМ М У  АЕР А МА А А ММ Л АРЛ ЛАЕ ЦЕРЕР,  М Е У МЕЖА  МРА Р МУ АЛЛА. А ЕУ РАА Л   МА У АЕР. Л АЕМГ ЕРГАА ЕА  АРАМ АЕРМ.  РА МЖЕ ГА , Р ЕЛ,   А ММЕ ,   ЛА,  ЕРМ М. , МЛРА Р МУ ЕА ААЛА Е  М АМ,  РЛ РМУА ЛЕ УЛАМ А  ЕЛЕ РАЕР,  АМА ЛУ АУ Е УЛ АЕРА. АЛЕ РМАЦ Р ЕР Е М А АМЕРАМУ ММУ ААРАУ ,  Л АЕР   РА.
	\end{minipage}
	
	\caption*{Вхідне джерело інформації} % Use \caption* to omit the label
	\label{fig:filtered-text}
\end{figure}

\subsection{Опис джерела інформації}
 
 \begin{table}[H]
 	\caption{Кількість появ символів $a_i$ $N(a_i)$ }
 	\centering 	
 	\input{latex_tables/occurrences.tex} 	
 \end{table}
  

Ймовірністі $P(a_i)$ появи симмолів первинного алфавіту визначаються за формулою
$$ P(a_i) = \frac{N(a_i)}{N}$$
де $N(a_i)$ - кількість появ символу $a_i$ у профільтрованому тексті, $N$ - кількість всіх символів у профільтрованому тексті

\begin{table}[H]
	\caption{Ймовірністі $P(a_i)$ появи симмолів первинного алфавіту}
	\centering
	\resizebox{\columnwidth}{!}{%
		\input{latex_tables/occurrence_probabilities.tex}
	}
\end{table}

Сума всіх ймовірностей $P(a_i)$ повинна дорівнювати одиниці, тобто
 
 $$ \sum_{i=1}^{|A|} P(a_i) = 1 $$

\subsection[Кодування]{Кодування}

Кількість бітів, необхідних для представлення числа $m_1$ у двійковій системі числення визначається як найменше ціле число, яке більше або рівне логарифму $m_1$ за основою $2$.
Визначимо кількість інформаційних розрядів $n_i $ за формулою 
$$n_i =\ceil[\big]{log_2 m_1} $$
$$n_i =\ceil[\big]{log_2 12}   =\ceil[\big]{3.584962500721156} = 4  $$
\begin{table}[H]
	\caption{Кодові комбінації $|B|$}
	\centering
	\input{latex_tables/codewords.tex}
\end{table}
Оскільки $|A| < |B|$ , то потрібно обрати 12 кодових комбінацій з шістнадцяти
можливих, які забезпечать більший рівень завадостійкості. \cite{hoek1992modified}

Для відбору набійльш завадостійких комбінацій використовується 
Відстань Геммінга (англ. Hamming distance)  — число позицій, у яких відповідні цифри двох двійкових слів однакової довжини різні. У загальнішому випадку відстань Геммінга застосовується для рядків однакової довжини будь-яких абеток, що складаються з $q$ символів, і служить метрикою відмінності (функцією, що визначає відстань в метричному просторі) об'єктів однакової вимірності. 

\begin{table}[H]
	\caption{Матриця кодових відстаней $d_{ij}$}
	\centering
		\resizebox{\columnwidth}{!}{%
		\input{latex_tables/distance_matrix.tex}
	}
\end{table}
\begin{table}[H]
	\caption{Аналіз кодових відстаней $d_{ij}$}
	\centering
	\input{latex_tables/distance_counts.tex}
	\label{table:codewords-distance-analysis}
\end{table}
Проведемо аналіз матриці кодових відстаней та покажемо результат цього
аналізу в додаткових в таблиці \ref{table:codewords-distance-analysis}.

На підставі аналізу цієї матриці вибираються кодові комбінації в кількості $|A|$, які
доцільно використовувати для завадостійкого кодування
інформації, та записується кодове відображення.

Аналіз матриці кодових відстаней показав, що кодові комбінації мають однаковий рівень завадостійкості.

У телекомунікаціях код Бергера — це односпрямований код виявлення помилок, названий на честь його винахідника Дж. М. Бергера. Коди Бергера можуть виявляти всі односпрямовані помилки. Односпрямовані помилки - це помилки, які перетворюють лише одиниці на нулі або лише нулі на одиниці, наприклад, в асиметричних каналах. Контрольні біти кодів Бергера обчислюються шляхом підрахунку всіх нулів в інформаційному слові та вираження цього числа в натуральній двійковій системі. Якщо інформаційне слово складається з $n$ бітів, тоді для коду Бергера потрібно 
$$k = \ceil[\big]{log_2 ( n + 1 )} $$  «перевірочнх біти», що додає Код Бергера довжини $k+n$. (Іншими словами,  $k$ контрольних бітів достатньо для перевірки до

 $$ n = 2 k - 1 n=2^{k}-1$$ 
інформаційних бітів). Коди Бергера можуть виявляти будь-яку кількість помилок перевертання бітів один до нуля, за умови, що в одному кодовому слові не було помилок нуль до одного. Подібним чином коди Бергера можуть виявляти будь-яку кількість помилок перетворення бітів від нуля до одного, за умови, що в одному кодовому слові не виникає помилок перетворення бітів від одного до нуля. Коди Бергера не можуть виправити жодної помилки.

Як і всі односпрямовані коди виявлення помилок, коди Бергера також можна використовувати в схемах, нечутливих до затримки.

Визначимо кількість контрольних бітів за формулою 
$$k = \ceil[\big]{log_2 ( n + 1 )} $$ 
$$k = \ceil[\big]{log_2 ( 4 + 1 )} = 3  $$ 
Обираємо перші 12 комбінацій із 16 можливих, оскільки завадостійкість є однаковою.  Для кожної кодової комбінації визначимо
кількість «1» в інформаційних розрядах та
запишемо її у двійковому виді:
\begin{table}[H]
	\caption{Симоли первинного алфавіту $A$, їх кодооові комбінації, та кількість «1» в кодовій комбінації $d_{ij}$}
	\centering
	\input{latex_tables/control_bits.tex}
	\label{table:codewords-control_bits}
\end{table}

Запишемо інвертовану кількість одиниць в
інформаційній частині кодової комбінації:

\begin{table}[H]
	\caption{Інвертована кількість «1» }
	\centering
	\input{latex_tables/control_bits_inverted.tex}
	\label{table:codewords-control_bits_inverterd}
\end{table}
Запишемо код Бергера.
Для цього допишемо праворуч до інформаційних
розрядів (пункт 1) перевірні розряди (пункт 3).

\begin{table}[H]
	\caption{Симоли первинного алфавіту $A$, та код Бергера}
	\centering
	\input{latex_tables/berger_code.tex}
	\label{table:berger-code}
\end{table}

\subsection{Визначення ймовірності $P_{\text{нп}} (f,A)$невиявлення помилок і оцінки інформаційних втрат $H(B/A)$ при передачі	інформації в каналі зв'язку}

Для визначення ймовірності $P_{\text{нп}} (f,A)$
невиявлення помилок і оцінки інформаційних втрат $H(B/A)$ при передачі інформації в каналі зв'язку створюється канальна матриця $P(b_j/a_i)$  для системи передачі інформації з вирішальним зворотним зв'язком, де значення в кожній комірці (крім комірок, що належать головній діагоналі) буде визначеновідповідно до формули
$$P(b_j/a_i) = p_e^{d_{ij}}(1-p_e)^{n-d_{ij}}, i \neq j$$
де $n$ - довжина кодової комбінації;
$d_{ij}$ - кодова відстань, що відповідає кодовим комбінаціям $a_i$ і $b_j$.

Значення в комірках головної діагоналі визначаються за формулою:
$$P(b_j/a_i) = 1- \sum_{j=1,j \neq 1}^{|A|} P(b_j/a_i) $$


\begin{table}[H]
	\caption{Канальна матриця $P( b_j / a_i )$}
	\centering
	\resizebox{\columnwidth}{!}{%
	\input{latex_tables/channel_matrix_p_bj_ai.tex}
}
	\label{table:channel_matrix_p_bj_ai}
\end{table}

Ймовірність невиявлення помилок для кодового зображення можна розрахувати за формулою:
$$P_{\text{нп}}(F(A))= \sum_{i=1}^{|A|}p(a_i) \cdot P_{\text{нп}}(a_i)$$ 
де $p(a_i) $ - ймовірність появи на виході джерела інформації символа $a_i$;
$P_{\text{нп}}(a_i)$ - ймовірність невиявлення помилок при передачі символа $a_i$

\begin{table}[H]
	\caption{Ймовірність невиявлення помилок $P_{\text{нп}}(a_i)$}
	\centering	
		\input{latex_tables/errors_probability.tex}	
	\label{table:errors_probability}
\end{table}
$$P_{\text{нп}}(F(A))= \sum_{i=1}^{|A|}p(a_i) \cdot P_{\text{нп}}(a_i) =\input{latex_tables/errors_probability_total.tex} $$ 

Ймовірність невиявленої помилки $P_{\text{нп}}$ для заданого кодового відображення $P_{\text{нп}} =0,101$


З метою зменшення ймовірності невиявлення помилок $P_{\text{нп}} (F(A))$ та інформаційних втрат $Н(В/А)$ здійснюють мінімізацію кодового зображення методом подвійного впорядковування за ймовірностями невиявлення помилок.

Для цього символу джерела повідомлення, що має найбільшу ймовірність появи $P(a_i)$, присвоюється кодова комбінація, яка має найменшу ймовірність невиявлення помилок $P_{\text{нп}} (a_i) $, а символу, що має найменшу ймовірність появи $P(a_i)$ , присвоюється кодова комбінація, яка має найбільшу ймовірність$P_{\text{нп}} (a_i) $.

Алгоритм мінімізації кодового зображення за ймовірностями невиявлення помилок полягає у наступному:
\begin{itemize}
	\item упорядковуються кодові комбінації за зменшенням ймовірностей $P_{\text{нп}} (a_i) $;
	\item упорядковуються символи первинного алфавіту за зростанням
	ймовірностей $P(a_i)$ їх появи;
	\item упорядкованій послідовності кодових комбінацій ставиться у відповідність упорядкована послідовність символів первинного алфавіту й записується мінімізоване кодове зображення $F_{min} (A)$.
\end{itemize}

В результаті мінімізації кодового зображення ймовірність невиявлення
помилок $P_{\text{нп}} (F(A))$ та інформаційні втрати $H(B/A)$, як правило, зменшуються.

\begin{table}[H]
	\caption{Мінімізація ймовірностей невиявлення помилок $P_{\text{нп}}(a_i)$}
	\centering	
	\input{latex_tables/minimization.tex}	
	\label{table:minimization}
\end{table}
\input{latex_tables/codes_min_expression}
\subsection{Оцінка інформаційних втрат $H(B/A)$ при передачі	інформації у каналі зв'язку;}

Для оцінки інформаційних втрат $H(B/A)$ при передачі інформації в каналі зв'язку за допомогою матриці $P(b_j / a_i)$ обчислюються часткові умовні ентропії $H(B/a_i)$ за формулою
$$H(B/a) = - \sum_{j=1}^{|A|} P(b_j / a_i) log_2 P(b_j / a_i) $$
Загальна умовна ентропія визначається згідно з виразом
$$H(B/A) =  \sum_{i=1}^{|A|} P(a_i)H(B/a_i)$$

\begin{table}[H]
	\caption{Часткові умовні ентропії $H(B/a_i)$}
	\centering	
	\resizebox{\columnwidth}{!}{%
	\input{latex_tables/conditional_entropy.tex}	
     }
	\label{table:conditional_entropy}
\end{table}

\begin{table}[H]
	\caption{Часткові умовні ентропії $H(B/a_i)$}
	\centering	
	\input{latex_tables/entropy_sums.tex}	
	\label{table:entropy_sums}
\end{table}

Результати розрахунку $H(B/A)$ наведемо у таблиці:

\begin{table}[H]
	\caption{Часткові умовні ентропії $H(B/a_i)$}
	\centering	
		\resizebox{\columnwidth}{!}{%
	\input{latex_tables/total_entropy.tex}	
}
	\label{table:total_entropy}
\end{table}
	\input{latex_tables/total_conditional_entropy.tex}	
\subsection{Висновок}
Більш завадостійкими є кодові комбінації, які характеризуються більшими
значеннями кодових відстаней.
\newpage
\section{Завдання 2}
Для заданого й описаного в завданні 1 джерела інформації необхідно:
\begin{itemize}
	\item  визначити теоретичний коефіцієнт стиску $\mu_{\text{теор}}$ 
	\item синтезувати кодове відображення відповідно до	заданого типу нерівномірного коду;
	\item визначити фактичний коефіцієнт стиску $\mu_{\text{факт}}$ 
	\item сформувати двійкову послідовність стислого інформаційного масиву для перших 20 символів профільтрованого тексту
	\item зробити висновки.
\end{itemize}

При передачі повідомлень, закодованих двійковим рівномірним кодом, не враховують статистичну структуру переданих повідомлень. Усі кодові комбінації при цьому мають однакову довжину. 

У теоремі \textbf{Шеннона про кодування повідомлень в каналах без завад} стверджується, що якщо передача дискретних повідомлень ведеться за
відсутності завад, то завжди можна знайти такий метод кодування, при якому середня кількість двійкових символів на одне повідомлення буде як завгодно близькою до ентропії джерела цих повідомлень, але ніколи не може бути менше її:
$$L_{\text{сер}} \geq H(A)$$
де $L_{\text{сер}}$ - середня довжина кодових комбінацій;\\
$H(A)$ – безумовна ентропія.
$$H(A) = - \sum_{i}P(a_i)log_2P(a_i)$$
де $P(a_i)$ – ймовірність появи символів $a_i$.
$$L_{\text{сер}} = \sum_{i}P(a_i)l(a_i)$$
де $l(a_i)$ – довжина кодової комбінації, що відповідає символу $a_i$.

Облік статистики повідомлень на підставі теореми Шеннона дозволяє будувати код, в якому повідомленням, що часто зустрічаються, присвоюються коротші кодові комбінації, а рідкісним ‒ довші. Такі коди називають нерівномірними.

Застосовують нерівномірні коди з метою стиснення інформації. Методи побудови таких кодів вперше запропонували одночасно в 1948-49 роках Р.Фано та К. Шеннон, тому код назвали кодом Шеннона-Фано.

\subsection{Алгоритм формування кодових комбінацій:}
\begin{enumerate}
	\item Повідомлення записуються в таблицю в порядку зменшення ймовірності їх появи
	\item Виконується розподіл на групи так щоб суми ймовірностей в кожній з груп були б по можливості однаковими
	\item Першій групі присвоюється значення «0», а другій ‒ значення «1».
	\item Повернення до пункту 2. Розподіл виконується до тих пір поки в кожній з груп не буде по одному символу.
	\item Записується кодове зображення.
\end{enumerate}

Основний принцип, покладений в основу кодування за методом Шеннона-Фано полягає в тому, що при виборі кожної цифри кодового слова прагнуть, щоб кількість інформації, яка міститься в ній, була найбільшою. Даний код є нерівномірним.

\begin{table}[H]
	\caption{Код Шенона}
	\centering	
	\input{latex_tables/shannon_encoding.tex}	
	\label{table:shannon_encoding}
\end{table}
Середня кількість символів на одне повідомлення $L_{\text{сер}}$ для розглянутого набору повідомлень становить: 


$2.94$

Ентропія даного коду:

$$H(A) = - \sum_{i}P(a_i)log_2P(a_i) \approx 2.869 \text{ біт/символ}$$ 

Таким чином $L_{\text{сер}} \geq H(A)$ 

Теоретичний коефіцієнт стиску $$\mu_{\text{теор}} = \frac{log_2(N)}{H(A) } = \frac{log_2(12)}{2.869} \approx 1.25$$

\begin{table}[H]
	\caption{Двійкова послідовність стислого інформаційного масиву для перших 20 символів профільтрованого тексту}
	\centering	
	\input{latex_tables/compressed.tex}	
	\label{table:compressed}
\end{table}
Фактичний коефіцієнт стиску $$\mu_{\text{факт}} = \frac{\text{Початковий розмір}}{\text{Розмір даних після стиску}} = \frac{4 *20 }{52}=  \approx 1.53$$

\printbibliography[heading=none]

\end{document}